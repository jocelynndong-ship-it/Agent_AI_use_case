
ğŸ¤– Multi-Agent AI Data Platform (BigQuery + Gemini)

![alt text](https://img.shields.io/badge/Python-3.9%2B-blue)
![alt text](https://img.shields.io/badge/Streamlit-1.30%2B-red)
![alt text](https://img.shields.io/badge/Google-BigQuery-4285F4)
![alt text](https://img.shields.io/badge/AI-Gemini%20Flash-8E75B2)

Une plateforme d'analyse de donnÃ©es autonome pilotÃ©e par des Agents IA.
Ce projet orchestre plusieurs scripts Python pour automatiser le pipeline de donnÃ©es : de l'ingestion (ETL) Ã  la gÃ©nÃ©ration de requÃªtes SQL complexes, en passant par le Reporting BI interactif et le Machine Learning automatisÃ© (Auto-ML).



-----------------------------------------
ğŸš€ FonctionnalitÃ©s ClÃ©s

ğŸ•µï¸â€â™‚ï¸ Agent SQL GÃ©nÃ©ratif : Transforme le langage naturel en SQL BigQuery optimisÃ©. Nettoie automatiquement les formats financiers ($, ,) et gÃ¨re les erreurs de syntaxe.

ğŸ”„ Pipeline ETL & Visualisation : ExÃ©cute les requÃªtes, nettoie les donnÃ©es (gestion des types pd.NA, dates), et gÃ©nÃ¨re une galerie de graphiques statiques (Matplotlib/Seaborn).

ğŸ“Š Dashboard BI Intelligent :

DÃ©tection automatique des coordonnÃ©es gÃ©ographiques (Cartes) ou des noms de lieux (Treemaps).

Graphiques interactifs (Plotly).

SystÃ¨me de cache intelligent (rechargement auto si la table change).

-----------------------------------------

ğŸ”® Auto-ML Lab (V7.1) :

Benchmark automatique de modÃ¨les (Random Forest, SVM, XGBoost, etc.).

Modes Classification, RÃ©gression et Clustering.

Analyse sÃ©mantique par l'IA pour suggÃ©rer la meilleure cible (Target) Ã  prÃ©dire.

-----------------------------------------

ğŸ“‚ Architecture du Projet

Le systÃ¨me est modulaire. Chaque script agit comme un agent spÃ©cialisÃ© :

Fichier	RÃ´le	Description

main.py	ğŸ® Chef d'Orchestre	Point d'entrÃ©e (CLI). GÃ¨re le menu, l'upload CSV vers BigQuery et lance les agents.

generate_kpi_query_g3.py	ğŸ§  Agent SQL	Analyse le schÃ©ma BigQuery et gÃ©nÃ¨re/corrige le SQL via Gemini. Sauvegarde dans generated_query.sql.

run_pipeline_g3.py	âš™ï¸ Orchestrateur	ExÃ©cute l'agent pipeline, gÃ¨re le nettoyage des anciens fichiers et la sauvegarde Parquet.

pipeline_agent.py	ğŸ¨ Agent Viz	ExÃ©cute la requÃªte, nettoie les donnÃ©es (Robustness) et crÃ©e les visuels statiques + rapport Markdown.

dashboard.py	ğŸ“Š Interface BI	Dashboard Streamlit complet (KPIs, Onglets dynamiques, GÃ©ospatial).

app3.py	ğŸ§ª Agent ML	Interface Auto-ML pour l'entraÃ®nement de modÃ¨les et l'analyse prÃ©dictive.


-----------------------------------------
ğŸ› ï¸ PrÃ©-requis

Google Cloud Platform (GCP) :

Un projet actif.

BigQuery API activÃ©e.

Un fichier de clÃ© de service (JSON) ou une authentification locale (gcloud auth application-default login).

-----------------------------------------
Gemini API :
Une clÃ© API valide (Google AI Studio).
Python 3.9+

-----------------------------------------
ğŸ“¦ Installation
Cloner le dÃ©pÃ´t :
git clone https://github.com/votre-user/votre-repo.git
cd votre-repo

-----------------------------------------
CrÃ©er un environnement virtuel :
python -m venv .venv
source .venv/bin/activate  # Mac/Linux


-----------------------------------------
Installer les dÃ©pendances :
pip install -r requirements.txt

Configurer les variables d'environnement :
CrÃ©ez un fichier .env Ã  la racine du projet :

PROJECT_ID=votre-projet-gcp-id
DATASET_ID=agent_dataset
GOOGLE_API_KEY=votre-cle-api-gemini


-----------------------------------------
â–¶ï¸ Utilisation
Lancez simplement le script principal pour dÃ©marrer l'assistant :
python main.py


Vous aurez accÃ¨s au menu interactif :

ğŸ“‚ Charger un fichier CSV local : Upload instantanÃ© vers BigQuery (crÃ©ation de table auto) puis lancement de l'analyse.

ğŸ—„ï¸ Utiliser une table existante : Analyse une table BigQuery dÃ©jÃ  prÃ©sente.

ğŸ”® Ouvrir l'Auto-ML Agent : AccÃ¨s direct au laboratoire de Machine Learning.

Une fois le pipeline terminÃ©, choisissez l'option 1 pour ouvrir le Dashboard Streamlit dans votre navigateur.



-----------------------------------------
ğŸ“¦ Requirements (DÃ©pendances)

Pour recrÃ©er le fichier requirements.txt, voici les librairies nÃ©cessaires :

streamlit

pandas

numpy

google-cloud-bigquery

google-generativeai

python-dotenv

matplotlib

seaborn

plotly

scikit-learn

pyarrow

db-dtypes



-----------------------------------------
ğŸ›¡ï¸ Robustesse & Gestion d'Erreurs

Ce projet a Ã©tÃ© conÃ§u pour la production :

Formatage NumÃ©rique : Correction automatique des erreurs SQL type Bad double value (virgules dans les nombres).

Cache Busting : Le Dashboard dÃ©tecte si la table cible a changÃ© et invalide le cache automatiquement.

Type Safety : Conversion forcÃ©e des types pd.NA (Nullables) pour Ã©viter les crashs de visualisation.

Clean Workspace : Suppression automatique des anciens graphiques/rapports avant chaque nouvelle exÃ©cution.


-----------------------------------------
ğŸ‘¤ Auteur
Jocelyn NDONG - Analyst Engineer (Devoteam G Cloud)
N'hÃ©sitez pas Ã  contribuer ou Ã  ouvrir une issue pour toute suggestion d'amÃ©lioration !




